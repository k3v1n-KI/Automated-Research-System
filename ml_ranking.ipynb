{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61143a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from urllib.parse import urlparse\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from firebase import db\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5c964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env and init Firebase\n",
    "dotenv = find_dotenv()\n",
    "if dotenv:\n",
    "    load_dotenv(dotenv)\n",
    "\n",
    "\n",
    "openai = OpenAI(api_key=os.getenv(\"OPENAI_RANDY_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2f2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— Helper Functions ———\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "def extract_features(result: dict, query: str) -> dict:\n",
    "    title, snippet, url = result[\"title\"], result[\"snippet\"], result[\"url\"]\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.lower()\n",
    "    tld = domain.split(\".\")[-1] if domain else \"\"\n",
    "    q_tokens = set(tokenize(query))\n",
    "    t_tokens = tokenize(title)\n",
    "    s_tokens = tokenize(snippet)\n",
    "    feats = {\n",
    "        \"title_overlap\":   sum(1 for w in q_tokens if w in t_tokens),\n",
    "        \"snippet_overlap\": sum(1 for w in q_tokens if w in s_tokens),\n",
    "        \"url_length\":      len(url),\n",
    "        \"url_depth\":       parsed.path.count(\"/\"),\n",
    "        \"has_date\":        int(any(y in title or y in snippet for y in (\"2023\",\"2024\"))),\n",
    "        \"score_trust\":     int(domain.endswith((\".gov\",\".org\",\".edu\"))),\n",
    "    }\n",
    "    # one-hot TLD\n",
    "    feats[f\"tld_{tld}\"] = 1\n",
    "    return feats\n",
    "\n",
    "def is_scrapable(url: str) -> bool:\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        return r.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_gpt_score(result: dict, query: str) -> int:\n",
    "    prompt = (\n",
    "        f\"Rate the relevance of this search result to the query:\\n\"\n",
    "        f\"Query: \\\"{query}\\\"\\n\"\n",
    "        f\"Title: \\\"{result['title']}\\\"\\n\"\n",
    "        f\"Snippet: \\\"{result['snippet']}\\\"\\n\"\n",
    "        f\"URL: {result['url']}\\n\\n\"\n",
    "        \"On a scale of 0 (irrelevant) to 5 (perfectly relevant), return a single integer.\"\n",
    "    )\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You rate search result relevance.\"},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    ).choices[0].message.content\n",
    "    # extract first integer\n",
    "    match = re.search(r\"\\d+\", resp)\n",
    "    if not match:\n",
    "        return 0\n",
    "    score = int(match.group())\n",
    "    return max(0, min(5, score))\n",
    "\n",
    "\n",
    "# ——— Data Fetching ———\n",
    "\n",
    "def fetch_ranking_data(collection: str = \"ranking_training_data\") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve all search results from Firestore collection.\n",
    "    Each document must have 'title', 'snippet', and 'url' fields.\n",
    "    \"\"\"\n",
    "    docs = db.collection(collection).stream()\n",
    "    results = []\n",
    "    for d in docs:\n",
    "        data = d.to_dict()\n",
    "        results.append({\n",
    "            \"title\":   data.get(\"title\", \"\"),\n",
    "            \"snippet\": data.get(\"snippet\", \"\"),\n",
    "            \"url\":     data.get(\"url\", \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# ——— Dataset Builders ———\n",
    "\n",
    "def build_ffnn_dataset(results: list[dict], query: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds a DataFrame of feedforward features + GPT score + scrapability.\n",
    "    Columns: all extracted features, 'gpt_score' (0–5), 'malformed_url' (0/1), 'label' placeholder.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for res in results:\n",
    "        feats = extract_features(res, query)\n",
    "        # scrapability\n",
    "        malformed = 0 if is_scrapable(res[\"url\"]) else 1\n",
    "        feats[\"malformed_url\"] = malformed\n",
    "        # GPT-based relevance score\n",
    "        feats[\"gpt_score\"] = 0 if malformed else get_gpt_score(res, query)\n",
    "        records.append(feats)\n",
    "    df = pd.DataFrame(records).fillna(0)\n",
    "    # one-hot any missing TLD columns\n",
    "    df = pd.get_dummies(df, columns=[c for c in df if c.startswith(\"tld_\")])\n",
    "    # placeholder label (to be set later)\n",
    "    df[\"label\"] = 0\n",
    "    return df\n",
    "\n",
    "def build_rnn_dataset(results: list[dict], query: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Builds an RNN-style dataset: raw text fields + GPT score + scrapability + placeholder label.\n",
    "    Each record is a dict with keys: 'query','title','snippet','gpt_score','malformed_url','label'\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for res in results:\n",
    "        malformed = 0 if is_scrapable(res[\"url\"]) else 1\n",
    "        score     = 0 if malformed else get_gpt_score(res, query)\n",
    "        dataset.append({\n",
    "            \"query\":        query,\n",
    "            \"title\":        res[\"title\"],\n",
    "            \"snippet\":      res[\"snippet\"],\n",
    "            \"malformed_url\":malformed,\n",
    "            \"gpt_score\":    score,\n",
    "            \"label\":        0\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ——— Firestore Storage ———\n",
    "\n",
    "def store_ffnn_dataset(df: pd.DataFrame, collection: str = \"ffnn_dataset\"):\n",
    "    \"\"\"\n",
    "    Stores each row of the FFNN DataFrame into Firestore.\n",
    "    Overwrites any existing documents with the same numeric ID.\n",
    "    \"\"\"\n",
    "    col = db.collection(collection)\n",
    "    # optional: clear existing\n",
    "    for d in col.stream():\n",
    "        col.document(d.id).delete()\n",
    "    for idx, row in df.reset_index(drop=True).iterrows():\n",
    "        data = row.to_dict()\n",
    "        col.document(str(idx)).set(data)\n",
    "\n",
    "def store_rnn_dataset(records: list[dict], collection: str = \"rnn_dataset\"):\n",
    "    \"\"\"\n",
    "    Stores each record of the RNN dataset into Firestore.\n",
    "    \"\"\"\n",
    "    col = db.collection(collection)\n",
    "    # optional: clear existing\n",
    "    for d in col.stream():\n",
    "        col.document(d.id).delete()\n",
    "    for idx, rec in enumerate(records):\n",
    "        col.document(str(idx)).set(rec)\n",
    "\n",
    "\n",
    "# ——— Combined Builder & Storer ———\n",
    "\n",
    "def build_and_store_datasets(\n",
    "    query: str,\n",
    "    ranking_collection: str = \"ranking_training_data\",\n",
    "    ffnn_collection:   str = \"ffnn_dataset\",\n",
    "    rnn_collection:    str = \"rnn_dataset\"\n",
    ") -> tuple[pd.DataFrame, list[dict]]:\n",
    "    \"\"\"\n",
    "    Fetches raw search results, builds both FFNN and RNN datasets,\n",
    "    and stores them in Firestore under the specified collections.\n",
    "    Returns (ffnn_df, rnn_records).\n",
    "    \"\"\"\n",
    "    results = fetch_ranking_data(ranking_collection)\n",
    "    ffnn_df  = build_ffnn_dataset(results, query)\n",
    "    rnn_data = build_rnn_dataset(results, query)\n",
    "\n",
    "    store_ffnn_dataset(ffnn_df, ffnn_collection)\n",
    "    store_rnn_dataset(rnn_data, rnn_collection)\n",
    "\n",
    "    return ffnn_df, rnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6bae14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built FFNN dataset shape: (4665, 129)\n",
      "Built RNN dataset size: 4665\n"
     ]
    }
   ],
   "source": [
    "query = \"Find a list of hospitals in Ontario\"\n",
    "ffnn_df, rnn_data = build_and_store_datasets(query)\n",
    "print(\"Built FFNN dataset shape:\", ffnn_df.shape)\n",
    "print(\"Built RNN dataset size:\", len(rnn_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_automated_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
